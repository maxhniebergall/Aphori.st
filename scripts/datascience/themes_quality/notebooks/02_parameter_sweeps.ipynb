{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Sweeps: N=K vs N=K+D Algorithm Comparison\n",
    "\n",
    "This notebook focuses on the core investigation: comparing N=K vs N=K+D algorithms using refined quality metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "\n",
    "# Add paths\n",
    "sys.path.append('../../puzzle-generation')\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Parameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run parameter sweep\n",
    "from generate_parameter_sweep import ParameterSweepGenerator\n",
    "\n",
    "async def run_focused_sweep():\n",
    "    \"\"\"Run focused algorithm comparison sweep\"\"\"\n",
    "    print(\"ðŸ”¬ Starting Focused Parameter Sweep\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Initialize sweep generator\n",
    "        sweep_gen = ParameterSweepGenerator()\n",
    "        await sweep_gen.initialize()\n",
    "        \n",
    "        # Run algorithm comparison (primary focus)\n",
    "        print(f\"\\n{'='*20} ALGORITHM COMPARISON SWEEP {'='*20}\")\n",
    "        await sweep_gen.run_algorithm_comparison_sweep()\n",
    "        \n",
    "        # Save results\n",
    "        sweep_gen.save_results()\n",
    "        \n",
    "        print(\"\\nâœ… Parameter sweep completed successfully!\")\n",
    "        return sweep_gen.results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Parameter sweep failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the sweep\n",
    "sweep_results = await run_focused_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze sweep results\n",
    "if sweep_results:\n",
    "    results_df = pd.DataFrame(sweep_results)\n",
    "    \n",
    "    print(\"ðŸ“Š Sweep Results Summary:\")\n",
    "    print(f\"Total attempts: {len(results_df)}\")\n",
    "    \n",
    "    successful_df = results_df[results_df['success'] == True]\n",
    "    print(f\"Successful: {len(successful_df)} ({len(successful_df)/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Group by algorithm\n",
    "    if 'algorithm' in results_df.columns:\n",
    "        print(\"\\nðŸ“ˆ Results by Algorithm:\")\n",
    "        for algorithm in results_df['algorithm'].unique():\n",
    "            alg_df = successful_df[successful_df['algorithm'] == algorithm]\n",
    "            if len(alg_df) > 0:\n",
    "                print(f\"\\n{algorithm}:\")\n",
    "                print(f\"  Success rate: {len(alg_df)}/{len(results_df[results_df['algorithm'] == algorithm])} \"\n",
    "                      f\"({len(alg_df)/len(results_df[results_df['algorithm'] == algorithm])*100:.1f}%)\")\n",
    "                \n",
    "                # Quality metrics\n",
    "                if 'refined_overall_quality_score' in alg_df.columns:\n",
    "                    avg_quality = alg_df['refined_overall_quality_score'].mean()\n",
    "                    std_quality = alg_df['refined_overall_quality_score'].std()\n",
    "                    print(f\"  Refined Quality: {avg_quality:.3f} Â± {std_quality:.3f}\")\n",
    "                \n",
    "                # Generation time\n",
    "                if 'generation_time' in alg_df.columns:\n",
    "                    avg_time = alg_df['generation_time'].mean()\n",
    "                    print(f\"  Avg Generation Time: {avg_time:.2f}s\")\n",
    "else:\n",
    "    print(\"âŒ No sweep results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality metrics comparison\n",
    "if sweep_results and len(successful_df) > 0:\n",
    "    \n",
    "    # Quality metric columns\n",
    "    quality_metrics = [\n",
    "        'refined_intracategory_word_distinctiveness',\n",
    "        'refined_intercategory_discoherence',\n",
    "        'refined_intracategory_coherence',\n",
    "        'refined_difficulty_progression',\n",
    "        'refined_overall_quality_score'\n",
    "    ]\n",
    "    \n",
    "    available_metrics = [m for m in quality_metrics if m in successful_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, metric in enumerate(available_metrics):\n",
    "            if i < len(axes):\n",
    "                # Box plot by algorithm\n",
    "                if 'algorithm' in successful_df.columns:\n",
    "                    successful_df.boxplot(column=metric, by='algorithm', ax=axes[i])\n",
    "                    axes[i].set_title(metric.replace('refined_', '').replace('_', ' ').title())\n",
    "                else:\n",
    "                    axes[i].hist(successful_df[metric], bins=10, alpha=0.7)\n",
    "                    axes[i].set_title(metric.replace('refined_', '').replace('_', ' ').title())\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for i in range(len(available_metrics), len(axes)):\n",
    "            axes[i].remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âŒ No quality metrics found in results\")\n",
    "else:\n",
    "    print(\"âŒ No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between algorithms\n",
    "if sweep_results and 'algorithm' in successful_df.columns and len(successful_df) > 0:\n",
    "    \n",
    "    algorithms = successful_df['algorithm'].unique()\n",
    "    \n",
    "    if len(algorithms) > 1:\n",
    "        print(\"ðŸ“Š Statistical Comparison Between Algorithms:\")\n",
    "        \n",
    "        # Quality metric comparison\n",
    "        if 'refined_overall_quality_score' in successful_df.columns:\n",
    "            print(\"\\nðŸŽ¯ Overall Quality Score:\")\n",
    "            for alg in algorithms:\n",
    "                alg_data = successful_df[successful_df['algorithm'] == alg]['refined_overall_quality_score']\n",
    "                print(f\"  {alg}: {alg_data.mean():.3f} Â± {alg_data.std():.3f} (n={len(alg_data)})\")\n",
    "        \n",
    "        # Generation time comparison  \n",
    "        if 'generation_time' in successful_df.columns:\n",
    "            print(\"\\nâ±ï¸ Generation Time:\")\n",
    "            for alg in algorithms:\n",
    "                alg_data = successful_df[successful_df['algorithm'] == alg]['generation_time']\n",
    "                print(f\"  {alg}: {alg_data.mean():.2f}s Â± {alg_data.std():.2f}s (n={len(alg_data)})\")\n",
    "        \n",
    "        # Statistical significance test (if scipy available)\n",
    "        try:\n",
    "            from scipy.stats import ttest_ind\n",
    "            \n",
    "            if len(algorithms) == 2 and 'refined_overall_quality_score' in successful_df.columns:\n",
    "                alg1_data = successful_df[successful_df['algorithm'] == algorithms[0]]['refined_overall_quality_score']\n",
    "                alg2_data = successful_df[successful_df['algorithm'] == algorithms[1]]['refined_overall_quality_score']\n",
    "                \n",
    "                if len(alg1_data) > 1 and len(alg2_data) > 1:\n",
    "                    t_stat, p_value = ttest_ind(alg1_data, alg2_data)\n",
    "                    print(f\"\\nðŸ“ˆ T-test between {algorithms[0]} and {algorithms[1]}:\")\n",
    "                    print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "                    print(f\"  p-value: {p_value:.3f}\")\n",
    "                    print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (Î± = 0.05)\")\n",
    "        except ImportError:\n",
    "            print(\"\\nâš ï¸ scipy not available for statistical tests\")\n",
    "    else:\n",
    "        print(\"ðŸ“Š Single algorithm tested - no comparison available\")\n",
    "else:\n",
    "    print(\"âŒ Insufficient data for statistical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Recent Results (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load most recent sweep results from file\n",
    "data_dir = Path('../data/raw')\n",
    "csv_files = list(data_dir.glob('parameter_sweep_*.csv'))\n",
    "\n",
    "if csv_files:\n",
    "    # Load most recent file\n",
    "    latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"ðŸ“ Loading results from: {latest_file.name}\")\n",
    "    \n",
    "    df = pd.read_csv(latest_file)\n",
    "    print(f\"   Total records: {len(df)}\")\n",
    "    print(f\"   Sweep types: {df['sweep_type'].unique() if 'sweep_type' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"ðŸ“ No existing sweep results found\")\n",
    "    print(\"   Run the parameter sweep first or use the cells above to generate new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook provides the framework for comparing N=K vs N=K+D algorithms using sophisticated quality metrics:\n",
    "\n",
    "### Key Comparisons\n",
    "1. **Word Distinctiveness**: How well each algorithm produces distinct words within categories\n",
    "2. **Category Separation**: Spatial separation between different puzzle categories  \n",
    "3. **Category Coherence**: Internal consistency and theme alignment\n",
    "4. **Performance**: Generation time and success rates\n",
    "\n",
    "### Next Steps\n",
    "1. Implement N=K+D algorithm variant in generator\n",
    "2. Run comprehensive comparisons with larger sample sizes\n",
    "3. Validate findings with human evaluation\n",
    "4. Optimize parameters based on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}