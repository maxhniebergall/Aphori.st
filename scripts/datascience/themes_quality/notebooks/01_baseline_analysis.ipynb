{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Analysis: Current Themes Generation System\n",
    "\n",
    "This notebook analyzes the current puzzle generation system to establish baseline performance metrics before optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add puzzle generation scripts to path\n",
    "sys.path.append('../../puzzle-generation')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load investigation configuration\n",
    "with open('../config/investigation_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "print(\"Investigation Configuration:\")\n",
    "print(f\"Name: {config['investigation']['name']}\")\n",
    "print(f\"Version: {config['investigation']['version']}\")\n",
    "print(f\"Description: {config['investigation']['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import puzzle generation components\n",
    "try:\n",
    "    from HighQualityPuzzleGenerator import HighQualityPuzzleGenerator\n",
    "    from FullVectorLoader import FullVectorLoader\n",
    "    from WordFrequencyService import WordFrequencyService\n",
    "    print(\"âœ… Successfully imported puzzle generation components\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import puzzle generation components: {e}\")\n",
    "    print(\"Please ensure the puzzle-generation scripts are available\")"
   ]
  ],
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize puzzle generation system\n",
    "print(\"ðŸš€ Initializing puzzle generation system...\")\n",
    "\n",
    "# Initialize vector loader\n",
    "vector_loader = FullVectorLoader()\n",
    "load_result = await vector_loader.initialize()\n",
    "\n",
    "if load_result['success']:\n",
    "    print(f\"âœ… Vector loader initialized: {load_result['loadedWords']} words loaded\")\n",
    "else:\n",
    "    print(\"âŒ Failed to initialize vector loader\")\n",
    "    \n",
    "# Initialize puzzle generator\n",
    "generator = HighQualityPuzzleGenerator(vector_loader)\n",
    "print(\"âœ… Puzzle generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Puzzle Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline puzzles with current parameters\n",
    "baseline_puzzles = []\n",
    "baseline_metrics = []\n",
    "\n",
    "num_baseline_samples = 20\n",
    "test_date = \"2024-08-05\"\n",
    "\n",
    "print(f\"ðŸŽ¯ Generating {num_baseline_samples} baseline puzzles...\")\n",
    "\n",
    "for i in range(num_baseline_samples):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate a single 4x4 puzzle for baseline\n",
    "        result = await generator.generateSinglePuzzle(test_date, i+1, 4)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        if result['puzzle']:\n",
    "            baseline_puzzles.append(result['puzzle'])\n",
    "            \n",
    "            # Collect metrics\n",
    "            metrics = {\n",
    "                'puzzle_id': i+1,\n",
    "                'generation_time': generation_time,\n",
    "                'quality_score': result['qualityScore'],\n",
    "                'attempts': result['attempts'],\n",
    "                'success': True,\n",
    "                'avg_similarity': result['puzzle']['metadata']['avgSimilarity'],\n",
    "                'num_categories': len(result['puzzle']['categories']),\n",
    "                'total_words': len(result['puzzle']['words'])\n",
    "            }\n",
    "            \n",
    "            # Add per-category metrics\n",
    "            for j, category in enumerate(result['puzzle']['categories']):\n",
    "                metrics[f'cat_{j+1}_difficulty'] = category['difficulty']\n",
    "                metrics[f'cat_{j+1}_similarity'] = category['similarity']\n",
    "                metrics[f'cat_{j+1}_theme'] = category['themeWord']\n",
    "                \n",
    "            baseline_metrics.append(metrics)\n",
    "            print(f\"   âœ… Puzzle {i+1}: Quality {result['qualityScore']:.3f}, Time {generation_time:.2f}s\")\n",
    "        else:\n",
    "            baseline_metrics.append({\n",
    "                'puzzle_id': i+1,\n",
    "                'generation_time': generation_time,\n",
    "                'success': False,\n",
    "                'attempts': result['attempts']\n",
    "            })\n",
    "            print(f\"   âŒ Puzzle {i+1}: Failed after {result['attempts']} attempts\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Puzzle {i+1}: Error - {e}\")\n",
    "        baseline_metrics.append({\n",
    "            'puzzle_id': i+1,\n",
    "            'generation_time': time.time() - start_time,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\nðŸ“Š Generated {len(baseline_puzzles)} successful puzzles out of {num_baseline_samples} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics to DataFrame for analysis\n",
    "baseline_df = pd.DataFrame(baseline_metrics)\n",
    "successful_df = baseline_df[baseline_df['success'] == True]\n",
    "\n",
    "print(\"ðŸ“ˆ Baseline Performance Summary:\")\n",
    "print(f\"Success Rate: {len(successful_df)}/{len(baseline_df)} ({len(successful_df)/len(baseline_df)*100:.1f}%)\")\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    print(f\"Average Quality Score: {successful_df['quality_score'].mean():.3f} Â± {successful_df['quality_score'].std():.3f}\")\n",
    "    print(f\"Average Generation Time: {successful_df['generation_time'].mean():.2f}s Â± {successful_df['generation_time'].std():.2f}s\")\n",
    "    print(f\"Average Similarity: {successful_df['avg_similarity'].mean():.3f} Â± {successful_df['avg_similarity'].std():.3f}\")\n",
    "    print(f\"Average Attempts: {successful_df['attempts'].mean():.1f} Â± {successful_df['attempts'].std():.1f}\")\n",
    "\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline metrics\n",
    "if len(successful_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Quality Score Distribution\n",
    "    axes[0,0].hist(successful_df['quality_score'], bins=10, alpha=0.7)\n",
    "    axes[0,0].set_title('Quality Score Distribution')\n",
    "    axes[0,0].set_xlabel('Quality Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Generation Time Distribution\n",
    "    axes[0,1].hist(successful_df['generation_time'], bins=10, alpha=0.7, color='orange')\n",
    "    axes[0,1].set_title('Generation Time Distribution')\n",
    "    axes[0,1].set_xlabel('Time (seconds)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Similarity vs Quality\n",
    "    axes[1,0].scatter(successful_df['avg_similarity'], successful_df['quality_score'], alpha=0.7)\n",
    "    axes[1,0].set_title('Similarity vs Quality Score')\n",
    "    axes[1,0].set_xlabel('Average Similarity')\n",
    "    axes[1,0].set_ylabel('Quality Score')\n",
    "    \n",
    "    # Attempts vs Quality\n",
    "    axes[1,1].scatter(successful_df['attempts'], successful_df['quality_score'], alpha=0.7, color='green')\n",
    "    axes[1,1].set_title('Generation Attempts vs Quality')\n",
    "    axes[1,1].set_xlabel('Attempts Required')\n",
    "    axes[1,1].set_ylabel('Quality Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ No successful puzzles to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze category-level metrics\n",
    "if len(successful_df) > 0:\n",
    "    category_data = []\n",
    "    \n",
    "    for _, row in successful_df.iterrows():\n",
    "        for cat_num in range(1, 5):  # 4x4 puzzles have 4 categories\n",
    "            if f'cat_{cat_num}_difficulty' in row:\n",
    "                category_data.append({\n",
    "                    'puzzle_id': row['puzzle_id'],\n",
    "                    'category': cat_num,\n",
    "                    'difficulty': row[f'cat_{cat_num}_difficulty'],\n",
    "                    'similarity': row[f'cat_{cat_num}_similarity'],\n",
    "                    'theme_word': row[f'cat_{cat_num}_theme']\n",
    "                })\n",
    "    \n",
    "    category_df = pd.DataFrame(category_data)\n",
    "    \n",
    "    print(\"ðŸ“Š Category Analysis:\")\n",
    "    print(category_df.groupby('category')[['difficulty', 'similarity']].agg(['mean', 'std']).round(3))\n",
    "    \n",
    "    # Plot category difficulty progression\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Difficulty progression\n",
    "    plt.subplot(1, 2, 1)\n",
    "    category_df.boxplot(column='difficulty', by='category', ax=plt.gca())\n",
    "    plt.title('Difficulty by Category Position')\n",
    "    plt.xlabel('Category Position')\n",
    "    plt.ylabel('Difficulty Level')\n",
    "    \n",
    "    # Similarity by category\n",
    "    plt.subplot(1, 2, 2) \n",
    "    category_df.boxplot(column='similarity', by='category', ax=plt.gca())\n",
    "    plt.title('Similarity by Category Position')\n",
    "    plt.xlabel('Category Position')\n",
    "    plt.ylabel('Minimum Similarity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ No successful puzzles to analyze categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words from successful puzzles for analysis\n",
    "if len(baseline_puzzles) > 0:\n",
    "    all_words = []\n",
    "    theme_words = []\n",
    "    \n",
    "    for puzzle in baseline_puzzles:\n",
    "        # Collect puzzle words\n",
    "        all_words.extend(puzzle['words'])\n",
    "        \n",
    "        # Collect theme words\n",
    "        for category in puzzle['categories']:\n",
    "            theme_words.append(category['themeWord'])\n",
    "    \n",
    "    print(f\"ðŸ“ Collected {len(all_words)} puzzle words and {len(theme_words)} theme words\")\n",
    "    \n",
    "    # Word length analysis\n",
    "    word_lengths = [len(word) for word in all_words]\n",
    "    theme_lengths = [len(word) for word in theme_words]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(word_lengths, bins=range(3, 12), alpha=0.7, label='Puzzle Words')\n",
    "    plt.hist(theme_lengths, bins=range(3, 12), alpha=0.7, label='Theme Words')\n",
    "    plt.title('Word Length Distribution')\n",
    "    plt.xlabel('Word Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Most common words\n",
    "    plt.subplot(1, 3, 2)\n",
    "    word_counts = pd.Series(all_words).value_counts().head(10)\n",
    "    word_counts.plot(kind='bar')\n",
    "    plt.title('Most Common Puzzle Words')\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Most common themes\n",
    "    plt.subplot(1, 3, 3)\n",
    "    theme_counts = pd.Series(theme_words).value_counts().head(10)\n",
    "    theme_counts.plot(kind='bar', color='orange')\n",
    "    plt.title('Most Common Theme Words')\n",
    "    plt.xlabel('Theme Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Word Statistics:\")\n",
    "    print(f\"Average puzzle word length: {np.mean(word_lengths):.1f} Â± {np.std(word_lengths):.1f}\")\n",
    "    print(f\"Average theme word length: {np.mean(theme_lengths):.1f} Â± {np.std(theme_lengths):.1f}\")\n",
    "    print(f\"Unique puzzle words: {len(set(all_words))} / {len(all_words)} ({len(set(all_words))/len(all_words)*100:.1f}%)\")\n",
    "    print(f\"Unique theme words: {len(set(theme_words))} / {len(theme_words)} ({len(set(theme_words))/len(theme_words)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"âŒ No successful puzzles to analyze words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline data for comparison\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save metrics\n",
    "baseline_df.to_csv(output_dir / f'baseline_metrics_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save successful puzzles\n",
    "if len(baseline_puzzles) > 0:\n",
    "    with open(output_dir / f'baseline_puzzles_{timestamp}.json', 'w') as f:\n",
    "        json.dump(baseline_puzzles, f, indent=2)\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_attempts': len(baseline_df),\n",
    "    'successful_puzzles': len(successful_df),\n",
    "    'success_rate': len(successful_df) / len(baseline_df) if len(baseline_df) > 0 else 0,\n",
    "    'avg_quality_score': successful_df['quality_score'].mean() if len(successful_df) > 0 else None,\n",
    "    'avg_generation_time': successful_df['generation_time'].mean() if len(successful_df) > 0 else None,\n",
    "    'avg_similarity': successful_df['avg_similarity'].mean() if len(successful_df) > 0 else None\n",
    "}\n",
    "\n",
    "with open(output_dir / f'baseline_summary_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Baseline results saved to {output_dir}\")\n",
    "print(f\"   - Metrics: baseline_metrics_{timestamp}.csv\")\n",
    "print(f\"   - Puzzles: baseline_puzzles_{timestamp}.json\") \n",
    "print(f\"   - Summary: baseline_summary_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This baseline analysis establishes the current performance characteristics of the themes puzzle generation system. Key findings will be used as reference points for parameter optimization in subsequent notebooks.\n",
    "\n",
    "### Next Steps\n",
    "1. **Parameter Sweeps**: Test different frequency and similarity thresholds\n",
    "2. **Algorithm Comparison**: Compare N=K vs alternative approaches  \n",
    "3. **Quality Assessment**: Develop automated quality scoring\n",
    "4. **Optimization**: Find optimal parameter combinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}