{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Analysis: Current Themes Generation System\n",
    "\n",
    "This notebook analyzes the current puzzle generation system using the refined quality metrics:\n",
    "- intracategory_word_distinctiveness\n",
    "- intercategory_discoherence\n",
    "- intracategory_coherence\n",
    "- difficulty_progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add puzzle generation scripts to path\n",
    "sys.path.append('../../puzzle-generation')\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load investigation configuration\n",
    "with open('../config/investigation_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "print(\"Investigation Configuration:\")\n",
    "print(f\"Name: {config['investigation']['name']}\")\n",
    "print(f\"Version: {config['investigation']['version']}\")\n",
    "print(f\"Quality Metrics: {', '.join(config['quality_assessment']['metrics'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import puzzle generation components\n",
    "try:\n",
    "    from HighQualityPuzzleGenerator import HighQualityPuzzleGenerator\n",
    "    from FullVectorLoader import FullVectorLoader\n",
    "    from WordFrequencyService import WordFrequencyService\n",
    "    print(\"‚úÖ Successfully imported puzzle generation components\")\n",
    "    \n",
    "    # Import quality metrics\n",
    "    from quality_metrics import QualityMetrics\n",
    "    print(\"‚úÖ Successfully imported quality metrics\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import components: {e}\")\n",
    "    print(\"Please ensure the puzzle-generation scripts are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize puzzle generation system\n",
    "print(\"üöÄ Initializing puzzle generation system...\")\n",
    "\n",
    "# Initialize vector loader\n",
    "vector_loader = FullVectorLoader()\n",
    "load_result = await vector_loader.initialize()\n",
    "\n",
    "if load_result['success']:\n",
    "    print(f\"‚úÖ Vector loader initialized: {load_result['loadedWords']} words loaded\")\n",
    "    \n",
    "    # Initialize puzzle generator\n",
    "    generator = HighQualityPuzzleGenerator(vector_loader)\n",
    "    print(\"‚úÖ Puzzle generator initialized\")\n",
    "    \n",
    "    # Initialize quality metrics with vector loader\n",
    "    quality_calculator = QualityMetrics(vector_loader)\n",
    "    print(\"‚úÖ Quality metrics initialized with vector support\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize vector loader\")\n",
    "    # Fallback to string-based metrics\n",
    "    quality_calculator = QualityMetrics()\n",
    "    print(\"‚ö†Ô∏è Using string-based quality metrics only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Baseline Puzzles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline puzzles with current parameters\n",
    "baseline_puzzles = []\n",
    "baseline_metrics = []\n",
    "baseline_quality_metrics = []\n",
    "\n",
    "num_baseline_samples = 25  # Increased for better statistics\n",
    "test_date = \"2024-08-05\"\n",
    "\n",
    "print(f\"üéØ Generating {num_baseline_samples} baseline puzzles with refined quality metrics...\")\n",
    "\n",
    "for i in range(num_baseline_samples):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate a single 4x4 puzzle for baseline\n",
    "        if 'generator' in locals():\n",
    "            result = await generator.generateSinglePuzzle(test_date, i+1, 4)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No generator available, using mock data\")\n",
    "            result = {'puzzle': None, 'qualityScore': 0, 'attempts': 0}\n",
    "            \n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        if result['puzzle']:\n",
    "            puzzle = result['puzzle']\n",
    "            baseline_puzzles.append(puzzle)\n",
    "            \n",
    "            # Calculate refined quality metrics\n",
    "            refined_metrics = quality_calculator.calculate_all_metrics(puzzle)\n",
    "            refined_metrics['puzzle_id'] = i + 1\n",
    "            baseline_quality_metrics.append(refined_metrics)\n",
    "            \n",
    "            # Collect basic metrics\n",
    "            metrics = {\n",
    "                'puzzle_id': i+1,\n",
    "                'generation_time': generation_time,\n",
    "                'quality_score': result['qualityScore'],\n",
    "                'attempts': result['attempts'],\n",
    "                'success': True,\n",
    "                'avg_similarity': puzzle['metadata']['avgSimilarity'],\n",
    "                'num_categories': len(puzzle['categories']),\n",
    "                'total_words': len(puzzle['words'])\n",
    "            }\n",
    "            \n",
    "            # Add per-category metrics\n",
    "            for j, category in enumerate(puzzle['categories']):\n",
    "                metrics[f'cat_{j+1}_difficulty'] = category['difficulty']\n",
    "                metrics[f'cat_{j+1}_similarity'] = category['similarity']\n",
    "                metrics[f'cat_{j+1}_theme'] = category['themeWord']\n",
    "                \n",
    "            baseline_metrics.append(metrics)\n",
    "            \n",
    "            print(f\"   ‚úÖ Puzzle {i+1}: Quality {result['qualityScore']:.3f}, \"\n",
    "                  f\"Refined Score {refined_metrics['overall_quality_score']:.3f}, \"\n",
    "                  f\"Time {generation_time:.2f}s\")\n",
    "        else:\n",
    "            baseline_metrics.append({\n",
    "                'puzzle_id': i+1,\n",
    "                'generation_time': generation_time,\n",
    "                'success': False,\n",
    "                'attempts': result['attempts']\n",
    "            })\n",
    "            print(f\"   ‚ùå Puzzle {i+1}: Failed after {result['attempts']} attempts\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Puzzle {i+1}: Error - {e}\")\n",
    "        baseline_metrics.append({\n",
    "            'puzzle_id': i+1,\n",
    "            'generation_time': time.time() - start_time,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\nüìä Generated {len(baseline_puzzles)} successful puzzles out of {num_baseline_samples} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Refined Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert refined quality metrics to DataFrame\n",
    "if baseline_quality_metrics:\n",
    "    quality_df = pd.DataFrame(baseline_quality_metrics)\n",
    "    \n",
    "    print(\"üìà Refined Quality Metrics Summary:\")\n",
    "    \n",
    "    metrics_to_analyze = [\n",
    "        'intracategory_word_distinctiveness',\n",
    "        'intercategory_discoherence', \n",
    "        'intracategory_coherence',\n",
    "        'difficulty_progression',\n",
    "        'overall_quality_score'\n",
    "    ]\n",
    "    \n",
    "    for metric in metrics_to_analyze:\n",
    "        if metric in quality_df.columns:\n",
    "            mean_val = quality_df[metric].mean()\n",
    "            std_val = quality_df[metric].std()\n",
    "            print(f\"{metric}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nüìã Sample Quality Metrics:\")\n",
    "    display(quality_df[metrics_to_analyze].head())\n",
    "else:\n",
    "    print(\"‚ùå No quality metrics calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot refined quality metrics\n",
    "if baseline_quality_metrics:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('intracategory_word_distinctiveness', 'Word Distinctiveness\\n(within categories)'),\n",
    "        ('intercategory_discoherence', 'Category Separation\\n(between categories)'),\n",
    "        ('intracategory_coherence', 'Category Coherence\\n(within categories)'),\n",
    "        ('difficulty_progression', 'Difficulty Progression'),\n",
    "        ('overall_quality_score', 'Overall Quality Score')\n",
    "    ]\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(metrics_to_plot):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        if metric in quality_df.columns:\n",
    "            axes[row, col].hist(quality_df[metric], bins=10, alpha=0.7, color=colors[i])\n",
    "            axes[row, col].set_title(title)\n",
    "            axes[row, col].set_xlabel('Score')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_val = quality_df[metric].mean()\n",
    "            axes[row, col].axvline(mean_val, color='red', linestyle='--', \n",
    "                                   label=f'Mean: {mean_val:.3f}')\n",
    "            axes[row, col].legend()\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[1, 2].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = quality_df[metrics_to_analyze].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Quality Metrics Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå No quality metrics to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Original vs Refined Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original quality scores with refined metrics\n",
    "if baseline_metrics and baseline_quality_metrics:\n",
    "    basic_df = pd.DataFrame(baseline_metrics)\n",
    "    successful_basic = basic_df[basic_df['success'] == True]\n",
    "    \n",
    "    if len(successful_basic) > 0 and len(quality_df) > 0:\n",
    "        # Merge dataframes\n",
    "        comparison_df = successful_basic.merge(quality_df, on='puzzle_id')\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Original vs Refined Quality Scores\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(comparison_df['quality_score'], comparison_df['overall_quality_score'], alpha=0.7)\n",
    "        plt.xlabel('Original Quality Score')\n",
    "        plt.ylabel('Refined Quality Score')\n",
    "        plt.title('Original vs Refined Quality')\n",
    "        plt.plot([0, 1], [0, 1], 'r--', alpha=0.5)  # diagonal line\n",
    "        \n",
    "        # Quality Score Distributions\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.hist(comparison_df['quality_score'], alpha=0.5, label='Original', bins=10)\n",
    "        plt.hist(comparison_df['overall_quality_score'], alpha=0.5, label='Refined', bins=10)\n",
    "        plt.xlabel('Quality Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Quality Score Distributions')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Generation Time vs Quality\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(comparison_df['generation_time'], comparison_df['overall_quality_score'], alpha=0.7)\n",
    "        plt.xlabel('Generation Time (seconds)')\n",
    "        plt.ylabel('Refined Quality Score')\n",
    "        plt.title('Time vs Quality')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = comparison_df['quality_score'].corr(comparison_df['overall_quality_score'])\n",
    "        print(f\"\\nüìä Correlation between original and refined quality scores: {correlation:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ùå Insufficient data for comparison\")\n",
    "else:\n",
    "    print(\"‚ùå Missing data for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline data for comparison\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save basic metrics\n",
    "if baseline_metrics:\n",
    "    basic_df = pd.DataFrame(baseline_metrics)\n",
    "    basic_df.to_csv(output_dir / f'baseline_basic_metrics_{timestamp}.csv', index=False)\n",
    "    print(f\"‚úÖ Saved basic metrics: baseline_basic_metrics_{timestamp}.csv\")\n",
    "\n",
    "# Save refined quality metrics\n",
    "if baseline_quality_metrics:\n",
    "    quality_df.to_csv(output_dir / f'baseline_quality_metrics_{timestamp}.csv', index=False)\n",
    "    print(f\"‚úÖ Saved quality metrics: baseline_quality_metrics_{timestamp}.csv\")\n",
    "\n",
    "# Save successful puzzles\n",
    "if baseline_puzzles:\n",
    "    with open(output_dir / f'baseline_puzzles_{timestamp}.json', 'w') as f:\n",
    "        json.dump(baseline_puzzles, f, indent=2)\n",
    "    print(f\"‚úÖ Saved puzzles: baseline_puzzles_{timestamp}.json\")\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'config': config,\n",
    "    'total_attempts': len(baseline_metrics),\n",
    "    'successful_puzzles': len(baseline_puzzles),\n",
    "    'success_rate': len(baseline_puzzles) / len(baseline_metrics) if baseline_metrics else 0,\n",
    "}\n",
    "\n",
    "if baseline_quality_metrics:\n",
    "    # Add refined quality metrics summary\n",
    "    for metric in ['intracategory_word_distinctiveness', 'intercategory_discoherence', \n",
    "                   'intracategory_coherence', 'difficulty_progression', 'overall_quality_score']:\n",
    "        if metric in quality_df.columns:\n",
    "            summary[f'avg_{metric}'] = float(quality_df[metric].mean())\n",
    "            summary[f'std_{metric}'] = float(quality_df[metric].std())\n",
    "\n",
    "with open(output_dir / f'baseline_summary_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved summary: baseline_summary_{timestamp}.json\")\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This baseline analysis establishes the current performance using refined quality metrics:\n",
    "\n",
    "### Key Findings\n",
    "1. **Word Distinctiveness**: Measures how different words are within each category\n",
    "2. **Category Separation**: Uses Calinski-Harabasz inspired metrics for inter-category discoherence\n",
    "3. **Category Coherence**: Measures within-category compactness and theme alignment\n",
    "4. **Difficulty Progression**: Validates systematic difficulty increase\n",
    "\n",
    "### Next Steps\n",
    "1. **Parameter Sweeps**: Test N=K vs N=K+D algorithms with these refined metrics\n",
    "2. **Optimization**: Find parameter combinations that maximize quality scores\n",
    "3. **Validation**: Compare results with human evaluation\n",
    "4. **Implementation**: Deploy optimized parameters to production system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}