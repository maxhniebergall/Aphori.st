#!/usr/bin/env python3
"""
ThemeProcessingTask - Individual task unit for multiprocessing pipeline

Each task represents a theme_word + candidate_words that needs Gemini embedding processing.
This class encapsulates the work unit and provides retry logic and error handling.
"""

import logging
import time
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class ThemeProcessingTask:
    """
    Individual processing task for a theme and its candidate words.
    
    This is the atomic unit of work in the multiprocessing system.
    Each task contains:
    - theme: The theme word to process
    - candidates: List of candidate words for this theme  
    - task_id: Unique identifier for tracking
    - retry_count: Number of retry attempts
    - max_retries: Maximum retry attempts allowed
    """
    
    theme: str
    candidates: List[str]
    task_id: str
    puzzle_id: int = 0
    theme_index: int = 0
    retry_count: int = 0
    max_retries: int = 3
    priority: int = 0  # Lower numbers = higher priority
    
    def __post_init__(self):
        """Validate task parameters after initialization."""
        if not self.theme or not self.theme.strip():
            raise ValueError("Theme cannot be empty")
        
        if not self.candidates:
            raise ValueError("Candidates list cannot be empty")
        
        if not self.task_id:
            # Generate task ID if not provided
            import uuid
            self.task_id = f"task_{uuid.uuid4().hex[:8]}"
    
    @property
    def can_retry(self) -> bool:
        """Check if this task can be retried."""
        return self.retry_count < self.max_retries
    
    def increment_retry(self):
        """Increment retry count."""
        self.retry_count += 1
    
    def reset_retries(self):
        """Reset retry count (used when task succeeds)."""
        self.retry_count = 0
    
    def to_dict(self) -> Dict:
        """Convert task to dictionary for serialization."""
        return {
            'task_id': self.task_id,
            'theme': self.theme,
            'candidates': self.candidates,
            'puzzle_id': self.puzzle_id,
            'theme_index': self.theme_index,
            'retry_count': self.retry_count,
            'max_retries': self.max_retries,
            'priority': self.priority
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'ThemeProcessingTask':
        """Create task from dictionary."""
        return cls(
            task_id=data['task_id'],
            theme=data['theme'],
            candidates=data['candidates'],
            puzzle_id=data.get('puzzle_id', 0),
            theme_index=data.get('theme_index', 0),
            retry_count=data.get('retry_count', 0),
            max_retries=data.get('max_retries', 3),
            priority=data.get('priority', 0)
        )
    
    def __str__(self) -> str:
        """String representation for logging."""
        return f"Task({self.task_id}): '{self.theme}' with {len(self.candidates)} candidates (retry: {self.retry_count}/{self.max_retries})"
    
    def __lt__(self, other):
        """Enable priority queue sorting (lower priority number = higher priority)."""
        if not isinstance(other, ThemeProcessingTask):
            return NotImplemented
        return self.priority < other.priority


@dataclass
class ThemeProcessingResult:
    """
    Result of processing a theme task.
    
    Contains the embeddings and similarities generated by Gemini API
    along with metadata about the processing.
    """
    
    task_id: str
    theme: str
    success: bool
    processing_time: float = 0.0
    error_message: Optional[str] = None
    
    # Gemini results (only present if success=True)
    theme_embedding: Optional[List[float]] = None
    word_embeddings: Optional[List[List[float]]] = None
    similarities: Optional[List[float]] = None
    selected_words: Optional[List[str]] = None
    
    # Processing metadata
    api_calls_made: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    
    def __post_init__(self):
        """Validate result data."""
        if self.success and not self.theme_embedding:
            raise ValueError("Successful result must include theme_embedding")
        
        if self.success and not self.selected_words:
            raise ValueError("Successful result must include selected_words")
    
    def to_dict(self) -> Dict:
        """Convert result to dictionary for serialization."""
        return {
            'task_id': self.task_id,
            'theme': self.theme,
            'success': self.success,
            'processing_time': self.processing_time,
            'error_message': self.error_message,
            'theme_embedding': self.theme_embedding,
            'word_embeddings': self.word_embeddings,
            'similarities': self.similarities,
            'selected_words': self.selected_words,
            'api_calls_made': self.api_calls_made,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'ThemeProcessingResult':
        """Create result from dictionary."""
        return cls(
            task_id=data['task_id'],
            theme=data['theme'],
            success=data['success'],
            processing_time=data.get('processing_time', 0.0),
            error_message=data.get('error_message'),
            theme_embedding=data.get('theme_embedding'),
            word_embeddings=data.get('word_embeddings'),
            similarities=data.get('similarities'),
            selected_words=data.get('selected_words'),
            api_calls_made=data.get('api_calls_made', 0),
            cache_hits=data.get('cache_hits', 0),
            cache_misses=data.get('cache_misses', 0)
        )
    
    @classmethod
    def create_error_result(cls, task_id: str, theme: str, error_message: str, processing_time: float = 0.0) -> 'ThemeProcessingResult':
        """Create an error result."""
        return cls(
            task_id=task_id,
            theme=theme,
            success=False,
            processing_time=processing_time,
            error_message=error_message
        )
    
    def __str__(self) -> str:
        """String representation for logging."""
        if self.success:
            return f"Result({self.task_id}): '{self.theme}' -> {len(self.selected_words)} words in {self.processing_time:.2f}s"
        else:
            return f"Result({self.task_id}): '{self.theme}' FAILED - {self.error_message}"


class TaskGenerator:
    """
    Utility class to generate ThemeProcessingTasks from themes and candidates data.
    """
    
    @staticmethod
    def create_tasks_from_pipeline_data(themes: List[str], candidates_dict: Dict, words_per_theme: int = 4) -> List[ThemeProcessingTask]:
        """
        Create processing tasks from pipeline data.
        
        Args:
            themes: List of theme words
            candidates_dict: Dictionary mapping themes to candidate words
            words_per_theme: Number of words needed per theme
        
        Returns:
            List of ThemeProcessingTask objects
        """
        tasks = []
        puzzle_id = 1
        theme_index = 0
        
        for theme in themes:
            if theme not in candidates_dict:
                logger.warning(f"No candidates found for theme: {theme}")
                continue
            
            candidate_words = candidates_dict[theme].get('words', [])
            if not candidate_words:
                logger.warning(f"Empty candidates list for theme: {theme}")
                continue
            
            # Create task for this theme
            task = ThemeProcessingTask(
                theme=theme,
                candidates=candidate_words,
                task_id=f"theme_{theme_index:03d}_{theme}",
                puzzle_id=puzzle_id,
                theme_index=theme_index,
                priority=theme_index  # Process in order initially
            )
            
            tasks.append(task)
            
            theme_index += 1
            # Group every 4 themes into a puzzle
            if theme_index % 4 == 0:
                puzzle_id += 1
        
        logger.info(f"Generated {len(tasks)} processing tasks for {len(themes)} themes")
        return tasks
    
    @staticmethod
    def create_tasks_for_puzzles(themes: List[str], candidates_dict: Dict, themes_per_puzzle: int = 4, total_puzzles: int = 20) -> List[ThemeProcessingTask]:
        """
        Create tasks organized by puzzle structure.
        
        Args:
            themes: List of theme words
            candidates_dict: Dictionary mapping themes to candidate words  
            themes_per_puzzle: Number of themes per puzzle (default 4)
            total_puzzles: Total number of puzzles to generate
        
        Returns:
            List of ThemeProcessingTask objects organized by puzzles
        """
        tasks = []
        themes_needed = total_puzzles * themes_per_puzzle
        
        # Extend themes by cycling if we don't have enough
        if len(themes) < themes_needed:
            extended_themes = []
            for i in range(themes_needed):
                extended_themes.append(themes[i % len(themes)])
            themes_to_use = extended_themes
        else:
            themes_to_use = themes[:themes_needed]
        
        # Create tasks grouped by puzzle
        for puzzle_id in range(1, total_puzzles + 1):
            start_idx = (puzzle_id - 1) * themes_per_puzzle
            puzzle_themes = themes_to_use[start_idx:start_idx + themes_per_puzzle]
            
            for theme_idx, theme in enumerate(puzzle_themes):
                if theme not in candidates_dict:
                    logger.warning(f"No candidates found for theme: {theme}")
                    continue
                
                candidate_words = candidates_dict[theme].get('words', [])
                if not candidate_words:
                    logger.warning(f"Empty candidates list for theme: {theme}")
                    continue
                
                # Create task with puzzle grouping
                task = ThemeProcessingTask(
                    theme=theme,
                    candidates=candidate_words,
                    task_id=f"p{puzzle_id:02d}_t{theme_idx}_{theme}",
                    puzzle_id=puzzle_id,
                    theme_index=theme_idx,
                    priority=puzzle_id * 10 + theme_idx  # Group by puzzle, then theme
                )
                
                tasks.append(task)
        
        logger.info(f"Generated {len(tasks)} processing tasks for {total_puzzles} puzzles ({themes_per_puzzle} themes each)")
        return tasks